{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345d9219",
   "metadata": {},
   "source": [
    "##### Week 18 Group Exercise\n",
    "###### Angela Spencer January 26, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62eaa5",
   "metadata": {},
   "source": [
    "##### 1. Look up the Adam optimization functions in PyTorch\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "##### How does it work? Try at least one other optimization function with the diabetes dataset shown in class. How does the model perform with the new optimizer? Did it perform better or worse than Adam? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e6a20",
   "metadata": {},
   "source": [
    "I created a loop to iterate through each optimizer function and print the classification report for the final epoch. I found that Adadelta and RMSprop were the only to opptimizers that out-performed Adam.  The other optimizers were slightly worse or the same for accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b74347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "diabetes_df = pd.read_csv(\"../Datasets/diabetes.csv\")\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "sc=StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c91d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#convert to tensors\n",
    "X_train = torch.FloatTensor(X_train) \n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train) \n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25620e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aritficial neural network\n",
    "#creating a class using the neural network module\n",
    "class ANN_Model(nn.Module):\n",
    "    \n",
    "    #uses the parameters for nn.Module, check documentation\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, out_features=2):\n",
    "        \n",
    "        # keyword super is a computed indirect reference, \n",
    "        # iolates changes and makes sure children in the layers of multiple inheritance\n",
    "        # are calling the correct parents\n",
    "        super().__init__() \n",
    "        \n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a03008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer <class 'torch.optim.adadelta.Adadelta'>: Epoch number 500 with loss, 0.4302569329738617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       150\n",
      "           1       0.66      0.54      0.59        81\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.69      0.70       231\n",
      "weighted avg       0.73      0.74      0.73       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adagrad.Adagrad'>: Epoch number 500 with loss, 0.08438178896903992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       150\n",
      "           1       0.46      0.51      0.48        81\n",
      "\n",
      "    accuracy                           0.62       231\n",
      "   macro avg       0.59      0.59      0.59       231\n",
      "weighted avg       0.63      0.62      0.62       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adam.Adam'>: Epoch number 500 with loss, 0.01809709146618843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       150\n",
      "           1       0.51      0.51      0.51        81\n",
      "\n",
      "    accuracy                           0.66       231\n",
      "   macro avg       0.62      0.62      0.62       231\n",
      "weighted avg       0.66      0.66      0.66       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adamw.AdamW'>: Epoch number 500 with loss, 0.02316361665725708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       150\n",
      "           1       0.50      0.49      0.50        81\n",
      "\n",
      "    accuracy                           0.65       231\n",
      "   macro avg       0.61      0.61      0.61       231\n",
      "weighted avg       0.65      0.65      0.65       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adamax.Adamax'>: Epoch number 500 with loss, 0.017312144860625267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.77      0.74       150\n",
      "           1       0.51      0.44      0.47        81\n",
      "\n",
      "    accuracy                           0.65       231\n",
      "   macro avg       0.61      0.61      0.61       231\n",
      "weighted avg       0.64      0.65      0.65       231\n",
      "\n",
      "Optimizer <class 'torch.optim.asgd.ASGD'>: Epoch number 500 with loss, 0.017296206206083298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.77      0.74       150\n",
      "           1       0.51      0.44      0.47        81\n",
      "\n",
      "    accuracy                           0.65       231\n",
      "   macro avg       0.61      0.61      0.61       231\n",
      "weighted avg       0.64      0.65      0.65       231\n",
      "\n",
      "Optimizer <class 'torch.optim.nadam.NAdam'>: Epoch number 500 with loss, 0.0075171515345573425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76       150\n",
      "           1       0.54      0.46      0.50        81\n",
      "\n",
      "    accuracy                           0.68       231\n",
      "   macro avg       0.64      0.63      0.63       231\n",
      "weighted avg       0.66      0.68      0.67       231\n",
      "\n",
      "Optimizer <class 'torch.optim.radam.RAdam'>: Epoch number 500 with loss, 3.8748748920625076e-05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.76       150\n",
      "           1       0.55      0.51      0.53        81\n",
      "\n",
      "    accuracy                           0.68       231\n",
      "   macro avg       0.65      0.64      0.64       231\n",
      "weighted avg       0.67      0.68      0.68       231\n",
      "\n",
      "Optimizer <class 'torch.optim.rmsprop.RMSprop'>: Epoch number 500 with loss, 0.22901517152786255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.65      0.73       150\n",
      "           1       0.54      0.75      0.63        81\n",
      "\n",
      "    accuracy                           0.68       231\n",
      "   macro avg       0.68      0.70      0.68       231\n",
      "weighted avg       0.73      0.68      0.69       231\n",
      "\n",
      "Optimizer <class 'torch.optim.rprop.Rprop'>: Epoch number 500 with loss, 0.0968044251203537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       150\n",
      "           1       0.51      0.65      0.57        81\n",
      "\n",
      "    accuracy                           0.66       231\n",
      "   macro avg       0.64      0.66      0.64       231\n",
      "weighted avg       0.68      0.66      0.67       231\n",
      "\n",
      "Optimizer <class 'torch.optim.sgd.SGD'>: Epoch number 500 with loss, 1.067876935005188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.91      0.76       150\n",
      "           1       0.35      0.09      0.14        81\n",
      "\n",
      "    accuracy                           0.62       231\n",
      "   macro avg       0.50      0.50      0.45       231\n",
      "weighted avg       0.54      0.62      0.54       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#create instance of model\n",
    "ann = ANN_Model()\n",
    "\n",
    "#loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# list of optimizers to evaluate\n",
    "optimizer_list = [torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.Adam, torch.optim.AdamW,\n",
    "                  torch.optim.Adamax, torch.optim.ASGD, torch.optim.NAdam, torch.optim.RAdam, \n",
    "                  torch.optim.RMSprop,torch.optim.Rprop, torch.optim.SGD]\n",
    "\n",
    "# for loop to explore different optimizers\n",
    "for x in optimizer_list:\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = x(ann.parameters(), lr=0.1)\n",
    "    \n",
    "    #run model through multiple epochs/iterations\n",
    "    final_loss = []\n",
    "    n_epochs = 501\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = ann.forward(X_train)\n",
    "        loss = loss_function(y_pred, y_train)\n",
    "        final_loss.append(loss)\n",
    "\n",
    "        if epoch == 500:\n",
    "            print(f'Optimizer {x}: Epoch number {epoch} with loss, {loss}')\n",
    "\n",
    "        # impliment optimizer\n",
    "        # gradient descent - zero the gradient before running backwards propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #perform optimization step for each epoch\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 500:\n",
    "            #predictions \n",
    "            y_pred = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(X_test):\n",
    "                    prediction = ann(data)\n",
    "                    y_pred.append(prediction.argmax())\n",
    "\n",
    "            print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2bc89",
   "metadata": {},
   "source": [
    "##### 2. Write a function that lists and counts the number of divisors for an input value.\n",
    "    Example 1:\n",
    "    Input: 5\n",
    "    Output: “There are 2 divisors: 1 and 5”\n",
    "    Example 2:\n",
    "    Input: 40\n",
    "    Output: “There are 8 divisors: 1, 2, 4, 5, 8, 10, 20, and 40”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30826d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b738034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = \", \"\n",
    "\n",
    "def factor_fun(num):\n",
    "    factors = []\n",
    "    for i in np.arange(1, num+1):\n",
    "        if num % i == 0:\n",
    "            factors.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    return f'There are {len(factors)} factors in {num}: {(separator.join(map(str, factors[:-1])))}, and {factors[-1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed20e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 2 factors in 5: 1, and 5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_fun(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b3b1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 8 factors in 40: 1, 2, 4, 5, 8, 10, 20, and 40'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_fun(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
